import { ChatMessage } from '../../../../lib/ollama';

/**
 * ä¸åŒæ¨¡å‹çš„Tokenä¼°ç®—é…ç½®
 */
interface ModelTokenConfig {
  charsPerToken: number;
  systemPromptWeight: number;
  memoryWeight: number;
  maxContextLength: number;
}

/**
 * é»˜è®¤æ¨¡å‹é…ç½®
 */
const DEFAULT_MODEL_CONFIGS: Record<string, ModelTokenConfig> = {
  // Qwenç³»åˆ—
  'qwen2.5:3b': { charsPerToken: 0.7, systemPromptWeight: 1.2, memoryWeight: 0.9, maxContextLength: 32768 },
  'qwen2.5:7b': { charsPerToken: 0.7, systemPromptWeight: 1.2, memoryWeight: 0.9, maxContextLength: 32768 },
  'qwen2.5:14b': { charsPerToken: 0.7, systemPromptWeight: 1.2, memoryWeight: 0.9, maxContextLength: 32768 },
  'qwen2.5:32b': { charsPerToken: 0.7, systemPromptWeight: 1.2, memoryWeight: 0.9, maxContextLength: 32768 },
  'qwen2.5:72b': { charsPerToken: 0.7, systemPromptWeight: 1.2, memoryWeight: 0.9, maxContextLength: 32768 },
  
  // LLaMAç³»åˆ—
  'llama3.1:8b': { charsPerToken: 0.8, systemPromptWeight: 1.1, memoryWeight: 0.9, maxContextLength: 131072 },
  'llama3.1:70b': { charsPerToken: 0.8, systemPromptWeight: 1.1, memoryWeight: 0.9, maxContextLength: 131072 },
  'llama3.1:405b': { charsPerToken: 0.8, systemPromptWeight: 1.1, memoryWeight: 0.9, maxContextLength: 131072 },
  
  // DeepSeekç³»åˆ—
  'deepseek-r1:1.5b': { charsPerToken: 0.75, systemPromptWeight: 1.15, memoryWeight: 0.85, maxContextLength: 131072 },
  'deepseek-r1:7b': { charsPerToken: 0.75, systemPromptWeight: 1.15, memoryWeight: 0.85, maxContextLength: 131072 },
  'deepseek-r1:8b': { charsPerToken: 0.75, systemPromptWeight: 1.15, memoryWeight: 0.85, maxContextLength: 131072 },
  'deepseek-r1:14b': { charsPerToken: 0.75, systemPromptWeight: 1.15, memoryWeight: 0.85, maxContextLength: 131072 },
  'deepseek-r1:32b': { charsPerToken: 0.75, systemPromptWeight: 1.15, memoryWeight: 0.85, maxContextLength: 131072 },
  'deepseek-r1:70b': { charsPerToken: 0.75, systemPromptWeight: 1.15, memoryWeight: 0.85, maxContextLength: 131072 },
  
  // é€šç”¨é»˜è®¤é…ç½®
  'default': { charsPerToken: 0.8, systemPromptWeight: 1.1, memoryWeight: 0.9, maxContextLength: 32768 }
};

/**
 * ä¸Šä¸‹æ–‡ä½¿ç”¨ç»Ÿè®¡
 */
export interface ContextUsage {
  totalTokens: number;
  systemTokens: number;
  memoryTokens: number;
  messageTokens: number;
  maxContextLength: number;
  usagePercentage: number;
  needsCleanup: boolean;
  recommendedKeepMessages: number;
}

/**
 * Tokenä¼°ç®—æœåŠ¡
 */
export class TokenEstimationService {
  /**
   * è·å–æ¨¡å‹é…ç½®
   */
  private static getModelConfig(model: string): ModelTokenConfig {
    // å°è¯•ç²¾ç¡®åŒ¹é…
    if (DEFAULT_MODEL_CONFIGS[model]) {
      return DEFAULT_MODEL_CONFIGS[model];
    }
    
    // å°è¯•æ¨¡ç³ŠåŒ¹é…
    const modelLower = model.toLowerCase();
    for (const [key, config] of Object.entries(DEFAULT_MODEL_CONFIGS)) {
      if (key !== 'default' && modelLower.includes(key.split(':')[0])) {
        return config;
      }
    }
    
    // è¿”å›é»˜è®¤é…ç½®
    return DEFAULT_MODEL_CONFIGS['default'];
  }

  /**
   * ä¼°ç®—å•æ¡æ¶ˆæ¯çš„Tokenæ•°é‡
   */
  static estimateMessageTokens(message: ChatMessage, model: string): number {
    const config = this.getModelConfig(model);
    const baseTokens = Math.ceil(message.content.length * config.charsPerToken);
    
    // æ ¹æ®æ¶ˆæ¯ç±»å‹è°ƒæ•´æƒé‡
    switch (message.role) {
      case 'system':
        return Math.ceil(baseTokens * config.systemPromptWeight);
      case 'user':
        return baseTokens;
      case 'assistant':
        return baseTokens;
      case 'tool':
        return Math.ceil(baseTokens * 0.8); // å·¥å…·æ¶ˆæ¯é€šå¸¸è¾ƒç®€æ´
      default:
        return baseTokens;
    }
  }

  /**
   * ä¼°ç®—å¤šæ¡æ¶ˆæ¯çš„æ€»Tokenæ•°é‡
   */
  static estimateMessagesTokens(messages: ChatMessage[], model: string): number {
    return messages.reduce((total, message) => {
      return total + this.estimateMessageTokens(message, model);
    }, 0);
  }

  /**
   * ä¼°ç®—è®°å¿†å†…å®¹çš„Tokenæ•°é‡
   */
  static estimateMemoryTokens(memoryContent: string, model: string): number {
    const config = this.getModelConfig(model);
    return Math.ceil(memoryContent.length * config.charsPerToken * config.memoryWeight);
  }

  /**
   * åˆ†æå½“å‰ä¸Šä¸‹æ–‡ä½¿ç”¨æƒ…å†µ
   */
  static analyzeContextUsage(
    messages: ChatMessage[],
    memoryContent: string,
    model: string
  ): ContextUsage {
    const config = this.getModelConfig(model);
    
    // åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œå…¶ä»–æ¶ˆæ¯
    const systemMessages = messages.filter(m => m.role === 'system');
    const otherMessages = messages.filter(m => m.role !== 'system');
    
    // è®¡ç®—å„éƒ¨åˆ†Tokenä½¿ç”¨é‡
    const systemTokens = this.estimateMessagesTokens(systemMessages, model);
    const memoryTokens = this.estimateMemoryTokens(memoryContent, model);
    const messageTokens = this.estimateMessagesTokens(otherMessages, model);
    
    const totalTokens = systemTokens + memoryTokens + messageTokens;
    const usagePercentage = (totalTokens / config.maxContextLength) * 100;
    
    // åˆ¤æ–­æ˜¯å¦éœ€è¦æ¸…ç†ï¼ˆä½¿ç”¨é‡è¶…è¿‡80%ï¼‰
    const needsCleanup = usagePercentage > 80;
    
    // è®¡ç®—å»ºè®®ä¿ç•™çš„æ¶ˆæ¯æ•°é‡ï¼ˆä¿ç•™æœ€æ–°çš„æ¶ˆæ¯ï¼Œç¡®ä¿ä¸è¶…è¿‡60%ä¸Šä¸‹æ–‡ï¼‰
    const targetTokens = Math.floor(config.maxContextLength * 0.6) - systemTokens - memoryTokens;
    let recommendedKeepMessages = 0;
    let accumulatedTokens = 0;
    
    // ä»æœ€æ–°æ¶ˆæ¯å¼€å§‹å‘å‰è®¡ç®—
    for (let i = otherMessages.length - 1; i >= 0; i--) {
      const messageTokens = this.estimateMessageTokens(otherMessages[i], model);
      if (accumulatedTokens + messageTokens <= targetTokens) {
        accumulatedTokens += messageTokens;
        recommendedKeepMessages++;
      } else {
        break;
      }
    }
    
    return {
      totalTokens,
      systemTokens,
      memoryTokens,
      messageTokens,
      maxContextLength: config.maxContextLength,
      usagePercentage,
      needsCleanup,
      recommendedKeepMessages
    };
  }

  /**
   * ä¼°ç®—ç”Ÿæˆè®°å¿†å¯èŠ‚çœçš„Tokenæ•°é‡
   */
  static estimateMemoryBenefit(
    messagesToSummarize: ChatMessage[],
    expectedSummaryLength: number,
    model: string
  ): number {
    const originalTokens = this.estimateMessagesTokens(messagesToSummarize, model);
    const summaryTokens = this.estimateMemoryTokens('x'.repeat(expectedSummaryLength), model);
    
    return Math.max(0, originalTokens - summaryTokens);
  }

  /**
   * è·å–æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
   */
  static getMaxContextLength(model: string): number {
    const config = this.getModelConfig(model);
    return config.maxContextLength;
  }

  /**
   * æ£€æŸ¥æ˜¯å¦éœ€è¦ç«‹å³è§¦å‘è®°å¿†ç”Ÿæˆï¼ˆåŸºäºä¸Šä¸‹æ–‡å‹åŠ›ï¼‰
   */
  static shouldTriggerMemoryByContext(
    messages: ChatMessage[],
    memoryContent: string,
    model: string
  ): boolean {
    const usage = this.analyzeContextUsage(messages, memoryContent, model);
    
    // å¦‚æœä¸Šä¸‹æ–‡ä½¿ç”¨é‡è¶…è¿‡75%ï¼Œå»ºè®®ç«‹å³è§¦å‘è®°å¿†ç”Ÿæˆ
    return usage.usagePercentage > 75;
  }

  /**
   * è®¡ç®—ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡é…ç½®
   */
  static calculateOptimalContext(
    messages: ChatMessage[],
    memoryContent: string,
    model: string
  ): {
    shouldCleanup: boolean;
    keepMessages: number;
    shouldGenerateMemory: boolean;
    messagesToSummarize: ChatMessage[];
  } {
    const usage = this.analyzeContextUsage(messages, memoryContent, model);
    const shouldCleanup = usage.needsCleanup;
    const keepMessages = usage.recommendedKeepMessages;
    
    // è¿‡æ»¤å‡ºéç³»ç»Ÿæ¶ˆæ¯
    const nonSystemMessages = messages.filter(m => m.role !== 'system');
    
    // å¦‚æœéœ€è¦æ¸…ç†ï¼Œè®¡ç®—éœ€è¦æ€»ç»“çš„æ¶ˆæ¯
    const messagesToSummarize = shouldCleanup 
      ? nonSystemMessages.slice(0, nonSystemMessages.length - keepMessages)
      : [];
    
    // å¦‚æœæœ‰è¶³å¤Ÿçš„æ¶ˆæ¯éœ€è¦æ€»ç»“ï¼Œå»ºè®®ç”Ÿæˆè®°å¿†
    const shouldGenerateMemory = messagesToSummarize.length > 4; // è‡³å°‘2è½®å¯¹è¯
    
    return {
      shouldCleanup,
      keepMessages,
      shouldGenerateMemory,
      messagesToSummarize
    };
  }

  /**
   * è·å–ä¸Šä¸‹æ–‡ä½¿ç”¨æƒ…å†µçš„å¯è¯»æŠ¥å‘Š
   */
  static getContextUsageReport(
    messages: ChatMessage[],
    memoryContent: string,
    model: string
  ): string {
    const usage = this.analyzeContextUsage(messages, memoryContent, model);
    
    return `ğŸ“Š ä¸Šä¸‹æ–‡ä½¿ç”¨æƒ…å†µæŠ¥å‘Šï¼š
- æ€»Tokenä½¿ç”¨é‡: ${usage.totalTokens}/${usage.maxContextLength} (${usage.usagePercentage.toFixed(1)}%)
- ç³»ç»Ÿæ¶ˆæ¯: ${usage.systemTokens} tokens
- è®°å¿†å†…å®¹: ${usage.memoryTokens} tokens  
- å¯¹è¯æ¶ˆæ¯: ${usage.messageTokens} tokens
- çŠ¶æ€: ${usage.needsCleanup ? 'âš ï¸ éœ€è¦æ¸…ç†' : 'âœ… æ­£å¸¸'}
- å»ºè®®ä¿ç•™æ¶ˆæ¯æ•°: ${usage.recommendedKeepMessages}`;
  }
} 